{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AI2 Codes.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fSTOJ3ak-6Gb","colab_type":"text"},"source":["#Inductive Learning\n","- Supervised learning: correct answers for each\n","example\n","\n","- Unsupervised learning: correct answers not given\n","\n","- Reinforcement learning: occasional rewards\n","\n","Simplest form - learn a function (model) from examples\n","\n","- Curve fitting\n","\n","Calculate via \n","- [Linear regression](https://www.tutorialspoint.com/statistics/linear_regression.htm)\n","\n","#[KNN](https://people.revoledu.com/kardi/tutorial/KNN/KNN_Numerical-example.html)\n"]},{"cell_type":"code","metadata":{"id":"VWn6vCepDzv_","colab_type":"code","colab":{}},"source":["\"\"\"\n","Created on Sun Dec 15 20:41:27 2019\n","\n","@author: CDS\n","\"\"\"\n","import pandas as pd\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split\n","\n","col_names = ['Y','X1','X2','X3','X4']\n","# load dataset\n","file = \"C:/Users/CDS/Desktop/FinalData.csv\"\n","pima = pd.read_csv(file, header=0,names=col_names)\n","pimaObj = pima.iloc[:,:].values\n","\n","#split dataset in features and target variable\n","feature_cols = ['X1','X2','X3','X4']\n","X = pima[feature_cols] # Features\n","target = ['Y']\n","y = pima[target] # Target variable\n","\n","X = X.astype('int')\n","y = y.astype('int')\n","\n","X_train,X_test, y_train, y_test = train_test_split(X, y,train_size = 6,shuffle = False) # 70% training and 30% test\n","\n","# Model class\n","model = KNeighborsClassifier(n_neighbors=3)\n","\n","# Train the model using the training sets\n","model.fit(X_train,y_train.values.ravel())\n","\n","#Predict Output\n","predicted= model.predict([[-1,1,1,-1]]) \n","print(predicted)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3uCA23G5HBn1","colab_type":"text"},"source":["#[Decision Tree](https://www.saedsayad.com/decision_tree.htm)\n","\n","Top down greedy search through space of possible branches with no backtracking\n","\n","##Entropy\n","Homogeneity of a sample from a 0-1 scaling\n","$$E(S) = \\sum^{c}_{i=1}-p_i log_2 p_i$$\n","Calculate entropy of all attributes\n","\n","##Information Gain\n","based on the decrease in entropy after a dataset is split on an attribute\n","- decision tree is all about finding attribute that returns the highest information gain\n","\n","1. Calculate Entropy of Target (Y)\n","2. The dataset is then split on the different attributes. The entropy for each branch is calculated. Then it is added proportionally, to get total entropy for the split. The resulting entropy is subtracted from the entropy before the split. The result is the Information Gain, or decrease in entropy.\n","  $$Gain(T,X)=E(T)-E(T,X)$$\n","  or\n","  $$Gain(T,X) = Target E - Branch E $$\n","3. Choose attribute with the largest information gain as the decision node, divide the dataset by its branches and repeat the same process on every branch.\n","\n","4. Branch with entropy of 0 is leaf node\n","5. Branch with entropy of more than 0 needs further splitting\n","6. Algorithm is run recursivly on the non leaf branches until all data is classified\n","\n","SKLearn Example"]},{"cell_type":"code","metadata":{"id":"8wpldNo_Kq7f","colab_type":"code","colab":{}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Wed Oct 16 18:47:53 2019\n","\n","@author: CDS\n","\"\"\"\n","\n","# Load libraries\n","import pandas as pd\n","from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n","from sklearn.model_selection import train_test_split # Import train_test_split function\n","from sklearn import metrics \n","from sklearn.tree import export_graphviz\n","from sklearn.externals.six import StringIO  \n","from IPython.display import Image  \n","import pydotplus\n","from sklearn.preprocessing import LabelEncoder\n","\n","from info_gain import info_gain\n","#Import scikit-learn metrics module for accuracy calculation\n","\n","col_names = ['Y','A1','A2','A3','A4']\n","# load dataset\n","pima = pd.read_csv(\"C:/Users/CDS/Desktop/FinalData.csv\", header=0,names=col_names)\n","pimaObj = pima.iloc[:,:].values\n","\n","#split dataset in features and target variable\n","feature_cols = ['A1','A2','A3','A4']\n","X = pima[feature_cols] # Features\n","target = ['Y']\n","y = pima[target] # Target variable\n","\n","X = X.astype('int')\n","y = y.astype('int')\n","\n","X_train,X_test, y_train, y_test = train_test_split(X, y,train_size = 6,shuffle = False) # 70% training and 30% test\n","\n","\n","ig = info_gain.info_gain(X,y)\n","\n","print(\"Information gain: \",ig)\n","\n","\n","# Create Decision Tree classifer object\n","clf = DecisionTreeClassifier(criterion = 'entropy')\n","\n","# Train Decision Tree Classifer\n","clf = clf.fit(X_train,y_train)\n","\n","predicted = clf.predict([[-1,-1,1,1]]) \n","print(predicted)\n","\n","#Predict the response for test dataset\n","#y_pred = clf.predict(X_test)\n","\n","# Model Accuracy, how often is the classifier correct?\n","#print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n","\n","dot_data = StringIO()\n","export_graphviz(clf, out_file=dot_data,  \n","                filled=True, rounded=True,\n","                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\n","graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n","graph.write_png('C:/Users/CDS/Desktop/HW1.png')\n","Image(graph.create_png())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aC5xtTpAOwZ-","colab_type":"text"},"source":["#Bayesian Probability\n","Density Estimator \n","- learns a mapping from a set of attributes to a Probability\n","\n","##Joint Distribution\n","Why using log in entropy? (2 reasons)\n","\n","Pros:\n","\n","A way to learn a density estimator from data\n","\n","Sort records by probability and find weird ones\n","(anomaly detection)\n","\n","Inference P(E1|E2)\n","\n","Foundation for Bayes Classifiers\n","\n","Cons? (over fitting: some combinations no sample)\n","\n","\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Ce0FvJupp4wq","colab":{}},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Tue Oct 22 16:51:09 2019\n","\n","@author: CDS\n","\"\"\"\n","\n","import pandas as pd\n","\n","#import data from CSV\n","col_names = ['play','outlook','temp','humidity','windy']\n","tennis = pd.read_csv(\"C:/Users/CDS/Desktop/FinalDataWords.csv\", header=0,names=col_names)\n","\n","#Organize data \n","outlook = tennis.groupby(['outlook', 'play']).size()\n","temp = tennis.groupby(['temp', 'play']).size()\n","humidity = tennis.groupby(['humidity', 'play']).size()\n","windy = tennis.groupby(['windy', 'play']).size()\n","play = tennis.play.value_counts()\n","\n","print(temp)\n","print('------------------')\n","print(humidity)\n","print('------------------')\n","print(windy)\n","print('------------------')\n","print(outlook)\n","print('------------------')\n","print('play')\n","print(play)\n","    \n","#case to find outlook,temp,humidity,windy\n","case = [-1,-1,1,1]\n","    \n","#output tables\n","\n","#X1\n","ct_outlook = pd.crosstab(tennis['outlook'], tennis['play'], margins = True)\n","print(ct_outlook)\n","\n","p_yes_outlook = ct_outlook.loc[case[0],['Yes']]/ct_outlook.loc[['All'],['Yes']]\n","p_no_outlook = ct_outlook.loc[case[0],['No']]/ct_outlook.loc[['All'],['No']]\n","\n","#X2\n","ct_temp = pd.crosstab(tennis['temp'], tennis['play'], margins = True)\n","print(ct_temp)\n","\n","p_yes_temp = ct_temp.loc[case[1],['Yes']]/ct_temp.loc[['All'],['Yes']]\n","p_no_temp = ct_temp.loc[case[1],['No']]/ct_temp.loc[['All'],['No']]\n","\n","ct_humid = pd.crosstab(tennis['humidity'], tennis['play'], margins = True)\n","print(ct_humid)\n","\n","p_yes_humid = ct_humid.loc[case[2],['Yes']]/ct_humid.loc[['All'],['Yes']]\n","p_no_humid = ct_humid.loc[case[2],['No']]/ct_humid.loc[['All'],['No']]\n","\n","#X3\n","ct_windy = pd.crosstab(tennis['windy'], tennis['play'], margins = True)\n","print(ct_windy)\n","\n","p_yes_windy = ct_windy.loc[case[3],['Yes']]/ct_windy.loc[['All'],['Yes']]\n","p_no_windy = ct_windy.loc[case[3],['No']]/ct_windy.loc[['All'],['No']]\n","\n","#X4\n","ct_play = pd.crosstab(index=tennis['play'],columns=\"count\", margins=True)\n","print(ct_play)\n","\n","p_yes = ct_play.at['Yes','All']/ct_play.at['All','All']\n","p_no = ct_play.at['No','All']/ct_play.at['All','All']\n","\n","# calculate prob of yes P(X|y=yes)\n","XYes = p_yes_outlook*p_yes_temp*p_yes_humid*p_yes_windy\n","XYes = XYes.at['All','Yes']\n","#calculate prob of no P(X|y=no)\n","XNo = p_no_outlook*p_no_temp*p_no_humid*p_no_windy\n","XNo = XNo.at['All','No']\n","#calculate y probs\n","\n","#calculate p_yes*yes and p_no*no which ever one is bigger is answer\n","print('Yes ',XYes*p_yes,'    No ',XNo*p_no)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v1_R8-cIqNCY","colab_type":"text"},"source":["##[Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n","\n","- Gaussian classifer used most frequently when data can be said to be normal dist\n","- Best used for classifying data that have strong independance assumptions between the features \n","- easily scalable\n","- Advantage: only requires a small number of training data to estimate the parameters necessary for classification\n","\n","No code -- \n","- sk-learn library has built in naive bayes function for gaussian dist and other classification. \n","\n","use logs if lots of floating point values (log likelihood)\n","\n","##[Bayes Network](https://towardsdatascience.com/introduction-to-bayesian-networks-81031eeed94e)\n","\n","- Markov Blanket\n","- Co Parents\n","\n","Useful for diagnosis, prediction, classification and decision making (given a cost function)\n","\n","##[Bayes Network Inference](https://en.wikipedia.org/wiki/Bayesian_inference)\n","\n","Bayes Inference\n","- used to update the probability of a hypothesis as more evidence or information becomes available\n","- Uses the bayesian posterior predictive distribution \n","\n","Bayesian theory calls for the use of the posterior predictive distribution to do predictive inference, i.e., to predict the distribution of a new, unobserved data point. That is, instead of a fixed point as a prediction, a distribution over possible points is returned\n","\n","Two Examples\n","- Maximum likelihood \n","- Maximum a Posteriori Estimation (MAP)\n","\n","##Variable Elimination\n","\n","##General Bayes Networks\n","- Generalization for any bayes network\n","\n","Decomposition\n","- Independent estimation problems\n","\n","Likelihood funciton : multinomials\n","\n","Dirichlet priors\n","\n","Learning Parameters: Summary\n","\n","##Learning Parameters: Incomplete data\n","\n","hidden nodes\n","\n","- Initial parameters\n","- Expectation via inference\n","- Maximization - update parameters\n","\n","Iterate until convergence\n","\n","##Learning Parameters : Graph Structure\n","\n","1.Heuristic search\n","- complete data - local computations\n","- incomplete data (score non-decomposable) - stochastic method\n","\n","2.Constrained-based methods (PC/IC algorithms)\n","\n","Scoring function: Minimum Description Length (MDL)\n","- Learning - data compression\n","\n","Model Selection Trade-offs\n","- Naive Bayes - too simple\n","- Unrestricted BN - too complex\n","- joint prob distribution is tree structured if a directed tree\n","\n","Optimal Trees: Chow-Liu result\n","- Lemma\n","- Theorem\n","- [Chow-Liu Algorithm](https://en.wikipedia.org/wiki/Chow%E2%80%93Liu_tree)\n","\n","\n","\n"," \n","\n","\n","\n"]}]}